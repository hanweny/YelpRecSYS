{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertConfig, DistilBertModel\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cAQMvg8vdIkXFAmFdmtf8Q</td>\n",
       "      <td>SJ7IbI1QVvia5muxByAv7w</td>\n",
       "      <td>3BzwagIBPQEf_Ic44oZtYQ</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Very upscale and romantic place. Good was exce...</td>\n",
       "      <td>2019-02-15 03:10:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sjm_uUcQVxab_EeLCqsYLg</td>\n",
       "      <td>0kA0PAJ8QFMeveQWHFqz2A</td>\n",
       "      <td>8zehGz9jnxPqXtOc7KaJxA</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The food is always great here. The service fro...</td>\n",
       "      <td>2011-07-28 18:05:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3_oBHUOlAC33347F8_vYKg</td>\n",
       "      <td>l8reACV0ZmCyjBZy4wtq-w</td>\n",
       "      <td>FxveeHL_B0Kkz1KjPKyF3A</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Literally the slowest and worst service I have...</td>\n",
       "      <td>2016-02-05 01:13:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gi5LSRmTXoL9Bp4jNGPjLw</td>\n",
       "      <td>hn0ZbitvmlHnF--KJGJ6_A</td>\n",
       "      <td>TA1KUSCu8GkWP9w0rmElxw</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I have been here twice and have had really goo...</td>\n",
       "      <td>2011-10-27 14:32:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R9SFR1FgssHATWd9PpQEHg</td>\n",
       "      <td>BjckP4AW2FXivEAUmh5d3g</td>\n",
       "      <td>PUZSvR-nEHlhEi0gSADu7w</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Went here with an friend visiting from Italy: ...</td>\n",
       "      <td>2017-09-10 16:16:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  cAQMvg8vdIkXFAmFdmtf8Q  SJ7IbI1QVvia5muxByAv7w  3BzwagIBPQEf_Ic44oZtYQ   \n",
       "1  sjm_uUcQVxab_EeLCqsYLg  0kA0PAJ8QFMeveQWHFqz2A  8zehGz9jnxPqXtOc7KaJxA   \n",
       "2  3_oBHUOlAC33347F8_vYKg  l8reACV0ZmCyjBZy4wtq-w  FxveeHL_B0Kkz1KjPKyF3A   \n",
       "3  Gi5LSRmTXoL9Bp4jNGPjLw  hn0ZbitvmlHnF--KJGJ6_A  TA1KUSCu8GkWP9w0rmElxw   \n",
       "4  R9SFR1FgssHATWd9PpQEHg  BjckP4AW2FXivEAUmh5d3g  PUZSvR-nEHlhEi0gSADu7w   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      5       0      0     0   \n",
       "1      4       0      0     0   \n",
       "2      1       2      2     2   \n",
       "3      4       0      0     0   \n",
       "4      4       0      0     0   \n",
       "\n",
       "                                                text                 date  \n",
       "0  Very upscale and romantic place. Good was exce...  2019-02-15 03:10:26  \n",
       "1  The food is always great here. The service fro...  2011-07-28 18:05:01  \n",
       "2  Literally the slowest and worst service I have...  2016-02-05 01:13:15  \n",
       "3  I have been here twice and have had really goo...  2011-10-27 14:32:57  \n",
       "4  Went here with an friend visiting from Italy: ...  2017-09-10 16:16:58  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"data/reviews2.csv\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Star Counts\n",
    "- Positive (star >= 4), Negative (star < 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stars count:   Counter({5: 722908, 4: 503086, 3: 234943, 2: 134182, 1: 112742})\n",
      "labels count:   Counter({1: 1225994, 0: 481867})\n"
     ]
    }
   ],
   "source": [
    "print(\"stars count:  \", Counter(reviews['stars']))\n",
    "reviews['label'] = 0\n",
    "reviews.loc[reviews['stars'] >= 4, 'label'] = 1\n",
    "print(\"labels count:  \", Counter(reviews['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-sampling each sentiment (5000 Each)\n",
    "- Using WordPiece to tokenize the strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(48)\n",
    "selected_idx = np.array([])\n",
    "for label in range(2):\n",
    "    selected_idx=np.append(selected_idx, np.random.choice(reviews[reviews['label'] == label].index, size=5000, replace=False), axis=0)\n",
    "    \n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "selected_text = reviews.loc[selected_idx, ['text', 'stars', 'label']]\n",
    "encoded_text = tokenizer(selected_text['text'].tolist(), padding='max_length', truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DistillBert Pre-training Weights\n",
    "- 10 Epochs\n",
    "- 1000 Batch (batch size: 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = DistilBertConfig()\n",
    "configuration.num_labels = 2\n",
    "configuration._name_or_path = 'distilbert-base-uncased'\n",
    "encoder = DistilBertForSequenceClassification(configuration)\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Batch: 0, time: 0.40135788917541504, loss: 0.6513352394104004\n",
      "Batch: 50, time: 0.2838094234466553, loss: 0.739883542060852\n",
      "Batch: 100, time: 0.285632848739624, loss: 0.7026276588439941\n",
      "Batch: 150, time: 0.2759861946105957, loss: 0.6837993860244751\n",
      "Batch: 200, time: 0.28923869132995605, loss: 0.6517378091812134\n",
      "Batch: 250, time: 0.2873342037200928, loss: 0.42001065611839294\n",
      "Batch: 300, time: 0.289722204208374, loss: 0.5119680166244507\n",
      "Batch: 350, time: 0.2913084030151367, loss: 0.4387996792793274\n",
      "Batch: 400, time: 0.28182029724121094, loss: 0.5223572850227356\n",
      "Batch: 450, time: 0.2658567428588867, loss: 0.27973473072052\n",
      "Batch: 500, time: 0.26551175117492676, loss: 0.6251158714294434\n",
      "Batch: 550, time: 0.2666037082672119, loss: 0.7200733423233032\n",
      "Batch: 600, time: 0.2940635681152344, loss: 0.3364659249782562\n",
      "Batch: 650, time: 0.2943398952484131, loss: 0.30579668283462524\n",
      "Batch: 700, time: 0.29401350021362305, loss: 0.3601791262626648\n",
      "Batch: 750, time: 0.29522180557250977, loss: 0.464717298746109\n",
      "Batch: 800, time: 0.26784300804138184, loss: 0.6491457223892212\n",
      "Batch: 850, time: 0.29339599609375, loss: 0.3752947747707367\n",
      "Batch: 900, time: 0.29621338844299316, loss: 0.47430285811424255\n",
      "Batch: 950, time: 0.2925143241882324, loss: 0.3362202048301697\n",
      "Average loss:  0.5394170536100864\n",
      "tensor(0.2849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Batch: 0, time: 0.2656116485595703, loss: 0.28208500146865845\n",
      "Batch: 50, time: 0.2977170944213867, loss: 0.6845886707305908\n",
      "Batch: 100, time: 0.2977604866027832, loss: 0.12877333164215088\n",
      "Batch: 150, time: 0.2974865436553955, loss: 0.4649520814418793\n",
      "Batch: 200, time: 0.2978992462158203, loss: 0.09999565035104752\n",
      "Batch: 250, time: 0.29737114906311035, loss: 0.06661634147167206\n",
      "Batch: 300, time: 0.28955626487731934, loss: 0.444282203912735\n",
      "Batch: 350, time: 0.27309203147888184, loss: 0.2636852264404297\n",
      "Batch: 400, time: 0.27453064918518066, loss: 0.19825592637062073\n",
      "Batch: 450, time: 0.29696083068847656, loss: 0.06911782920360565\n",
      "Batch: 500, time: 0.27480602264404297, loss: 0.5164107084274292\n",
      "Batch: 550, time: 0.2799041271209717, loss: 0.5348681211471558\n",
      "Batch: 600, time: 0.3010709285736084, loss: 0.14328905940055847\n",
      "Batch: 650, time: 0.29459714889526367, loss: 0.22202754020690918\n",
      "Batch: 700, time: 0.3029336929321289, loss: 0.35298991203308105\n",
      "Batch: 750, time: 0.30151820182800293, loss: 0.5407787561416626\n",
      "Batch: 800, time: 0.3020918369293213, loss: 0.3706320822238922\n",
      "Batch: 850, time: 0.30080461502075195, loss: 0.21232791244983673\n",
      "Batch: 900, time: 0.3004920482635498, loss: 0.3164704144001007\n",
      "Batch: 950, time: 0.29854679107666016, loss: 0.28212785720825195\n",
      "Average loss:  0.3561969303190708\n",
      "tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Batch: 0, time: 0.291093111038208, loss: 0.24080243706703186\n",
      "Batch: 50, time: 0.2960522174835205, loss: 0.7389264106750488\n",
      "Batch: 100, time: 0.2743830680847168, loss: 0.09048601984977722\n",
      "Batch: 150, time: 0.27568531036376953, loss: 0.31728821992874146\n",
      "Batch: 200, time: 0.2732968330383301, loss: 0.11407432705163956\n",
      "Batch: 250, time: 0.29906177520751953, loss: 0.033992838114500046\n",
      "Batch: 300, time: 0.3017001152038574, loss: 0.3073514699935913\n",
      "Batch: 350, time: 0.2710425853729248, loss: 0.22163133323192596\n",
      "Batch: 400, time: 0.302701473236084, loss: 0.07809492200613022\n",
      "Batch: 450, time: 0.30401110649108887, loss: 0.036091696470975876\n",
      "Batch: 500, time: 0.27448415756225586, loss: 0.2373884916305542\n",
      "Batch: 550, time: 0.30400705337524414, loss: 0.47942495346069336\n",
      "Batch: 600, time: 0.30014705657958984, loss: 0.09111423790454865\n",
      "Batch: 650, time: 0.2989847660064697, loss: 0.08812499046325684\n",
      "Batch: 700, time: 0.3029618263244629, loss: 0.16447710990905762\n",
      "Batch: 750, time: 0.2962496280670166, loss: 0.39505869150161743\n",
      "Batch: 800, time: 0.29848194122314453, loss: 0.34766921401023865\n",
      "Batch: 850, time: 0.28860020637512207, loss: 0.11614318192005157\n",
      "Batch: 900, time: 0.27289581298828125, loss: 0.3095865249633789\n",
      "Batch: 950, time: 0.30051326751708984, loss: 0.29625406861305237\n",
      "Average loss:  0.2901464802380651\n",
      "tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Batch: 0, time: 0.2938802242279053, loss: 0.2173796445131302\n",
      "Batch: 50, time: 0.2720930576324463, loss: 0.6479376554489136\n",
      "Batch: 100, time: 0.3025815486907959, loss: 0.05871816352009773\n",
      "Batch: 150, time: 0.29004335403442383, loss: 0.2541305422782898\n",
      "Batch: 200, time: 0.30187129974365234, loss: 0.05458585545420647\n",
      "Batch: 250, time: 0.3015010356903076, loss: 0.02500101365149021\n",
      "Batch: 300, time: 0.30190443992614746, loss: 0.15993736684322357\n",
      "Batch: 350, time: 0.301638126373291, loss: 0.20819219946861267\n",
      "Batch: 400, time: 0.30027198791503906, loss: 0.1425747573375702\n",
      "Batch: 450, time: 0.30008721351623535, loss: 0.024852503091096878\n",
      "Batch: 500, time: 0.2935945987701416, loss: 0.0695536881685257\n",
      "Batch: 550, time: 0.2994506359100342, loss: 0.440920889377594\n",
      "Batch: 600, time: 0.2730712890625, loss: 0.10803233087062836\n",
      "Batch: 650, time: 0.27853822708129883, loss: 0.13431474566459656\n",
      "Batch: 700, time: 0.3005385398864746, loss: 0.2255113422870636\n",
      "Batch: 750, time: 0.27360033988952637, loss: 0.4960281252861023\n",
      "Batch: 800, time: 0.2753481864929199, loss: 0.19389331340789795\n",
      "Batch: 850, time: 0.30023622512817383, loss: 0.09634435176849365\n",
      "Batch: 900, time: 0.28566813468933105, loss: 0.3003389239311218\n",
      "Batch: 950, time: 0.2735564708709717, loss: 0.3103879690170288\n",
      "Average loss:  0.247986683094874\n",
      "tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Batch: 0, time: 0.2901325225830078, loss: 0.3222165107727051\n",
      "Batch: 50, time: 0.299713134765625, loss: 0.7957622408866882\n",
      "Batch: 100, time: 0.3027017116546631, loss: 0.06000831723213196\n",
      "Batch: 150, time: 0.2995336055755615, loss: 0.05270444601774216\n",
      "Batch: 200, time: 0.29334568977355957, loss: 0.03893817216157913\n",
      "Batch: 250, time: 0.27309298515319824, loss: 0.02157888375222683\n",
      "Batch: 300, time: 0.29524779319763184, loss: 0.08684058487415314\n",
      "Batch: 350, time: 0.29428815841674805, loss: 0.19866125285625458\n",
      "Batch: 400, time: 0.27178478240966797, loss: 0.09262971580028534\n",
      "Batch: 450, time: 0.3006460666656494, loss: 0.024571608752012253\n",
      "Batch: 500, time: 0.2983431816101074, loss: 0.032712291926145554\n",
      "Batch: 550, time: 0.2727818489074707, loss: 0.6597583293914795\n",
      "Batch: 600, time: 0.3021082878112793, loss: 0.09754617512226105\n",
      "Batch: 650, time: 0.3005681037902832, loss: 0.03607888147234917\n",
      "Batch: 700, time: 0.300032377243042, loss: 0.09257006645202637\n",
      "Batch: 750, time: 0.3015611171722412, loss: 0.45554399490356445\n",
      "Batch: 800, time: 0.30211877822875977, loss: 0.33170053362846375\n",
      "Batch: 850, time: 0.29925990104675293, loss: 0.07510112971067429\n",
      "Batch: 900, time: 0.28898024559020996, loss: 0.3904417157173157\n",
      "Batch: 950, time: 0.3006730079650879, loss: 0.15924625098705292\n",
      "Average loss:  0.21809067528601736\n",
      "tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Batch: 0, time: 0.26976799964904785, loss: 0.19877569377422333\n",
      "Batch: 50, time: 0.2947533130645752, loss: 0.6364800333976746\n",
      "Batch: 100, time: 0.304063081741333, loss: 0.03215819597244263\n",
      "Batch: 150, time: 0.2726876735687256, loss: 0.13227787613868713\n",
      "Batch: 200, time: 0.29078054428100586, loss: 0.027396678924560547\n",
      "Batch: 250, time: 0.3006317615509033, loss: 0.020936142653226852\n",
      "Batch: 300, time: 0.30002522468566895, loss: 0.12753820419311523\n",
      "Batch: 350, time: 0.2938518524169922, loss: 0.45485275983810425\n",
      "Batch: 400, time: 0.2995131015777588, loss: 0.059528667479753494\n",
      "Batch: 450, time: 0.2997767925262451, loss: 0.022045601159334183\n",
      "Batch: 500, time: 0.2993507385253906, loss: 0.021193956956267357\n",
      "Batch: 550, time: 0.3003830909729004, loss: 0.634385347366333\n",
      "Batch: 600, time: 0.300764799118042, loss: 0.09391337633132935\n",
      "Batch: 650, time: 0.29863691329956055, loss: 0.04581158235669136\n",
      "Batch: 700, time: 0.2890770435333252, loss: 0.03302966430783272\n",
      "Batch: 750, time: 0.3029658794403076, loss: 0.30109745264053345\n",
      "Batch: 800, time: 0.28400659561157227, loss: 0.29199737310409546\n",
      "Batch: 850, time: 0.2981584072113037, loss: 0.06782327592372894\n",
      "Batch: 900, time: 0.2717289924621582, loss: 0.14208897948265076\n",
      "Batch: 950, time: 0.2987861633300781, loss: 0.1852143257856369\n",
      "Average loss:  0.18562637275271118\n",
      "tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Batch: 0, time: 0.29024267196655273, loss: 0.1383357048034668\n",
      "Batch: 50, time: 0.29690098762512207, loss: 0.5231994390487671\n",
      "Batch: 100, time: 0.3005087375640869, loss: 0.028309155255556107\n",
      "Batch: 150, time: 0.30155086517333984, loss: 0.015206563286483288\n",
      "Batch: 200, time: 0.29475855827331543, loss: 0.01850137673318386\n",
      "Batch: 250, time: 0.2820394039154053, loss: 0.030568862333893776\n",
      "Batch: 300, time: 0.2737696170806885, loss: 0.09968478232622147\n",
      "Batch: 350, time: 0.30199456214904785, loss: 0.34499508142471313\n",
      "Batch: 400, time: 0.27242255210876465, loss: 0.06086235120892525\n",
      "Batch: 450, time: 0.2836296558380127, loss: 0.01439044438302517\n",
      "Batch: 500, time: 0.2998807430267334, loss: 0.015846122056245804\n",
      "Batch: 550, time: 0.27411651611328125, loss: 0.6252881288528442\n",
      "Batch: 600, time: 0.2733340263366699, loss: 0.032108258455991745\n",
      "Batch: 650, time: 0.2976255416870117, loss: 0.02082807943224907\n",
      "Batch: 700, time: 0.29239749908447266, loss: 0.03773832321166992\n",
      "Batch: 750, time: 0.3006138801574707, loss: 0.39532941579818726\n",
      "Batch: 800, time: 0.2948944568634033, loss: 0.3003165125846863\n",
      "Batch: 850, time: 0.29468870162963867, loss: 0.03583396598696709\n",
      "Batch: 900, time: 0.27246713638305664, loss: 0.13467469811439514\n",
      "Batch: 950, time: 0.30109310150146484, loss: 0.05165152624249458\n",
      "Average loss:  0.1603405413837172\n",
      "tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Batch: 0, time: 0.277515172958374, loss: 0.04794124513864517\n",
      "Batch: 50, time: 0.2705049514770508, loss: 0.5850919485092163\n",
      "Batch: 100, time: 0.30269336700439453, loss: 0.03916385769844055\n",
      "Batch: 150, time: 0.30013084411621094, loss: 0.00829548854380846\n",
      "Batch: 200, time: 0.27158403396606445, loss: 0.026945292949676514\n",
      "Batch: 250, time: 0.3013448715209961, loss: 0.008984064683318138\n",
      "Batch: 300, time: 0.3024308681488037, loss: 0.1277165412902832\n",
      "Batch: 350, time: 0.3011033535003662, loss: 0.09091649204492569\n",
      "Batch: 400, time: 0.2865786552429199, loss: 0.07308278232812881\n",
      "Batch: 450, time: 0.2962517738342285, loss: 0.013433849439024925\n",
      "Batch: 500, time: 0.2990586757659912, loss: 0.009045029059052467\n",
      "Batch: 550, time: 0.29999637603759766, loss: 0.6186514496803284\n",
      "Batch: 600, time: 0.27831125259399414, loss: 0.026912832632660866\n",
      "Batch: 650, time: 0.27227234840393066, loss: 0.018302882090210915\n",
      "Batch: 700, time: 0.29845690727233887, loss: 0.17370440065860748\n",
      "Batch: 750, time: 0.2724578380584717, loss: 0.11206555366516113\n",
      "Batch: 800, time: 0.2996938228607178, loss: 0.38601431250572205\n",
      "Batch: 850, time: 0.298980712890625, loss: 0.03026597574353218\n",
      "Batch: 900, time: 0.28882288932800293, loss: 0.06328152120113373\n",
      "Batch: 950, time: 0.27291274070739746, loss: 0.07339943200349808\n",
      "Average loss:  0.13888819506205619\n",
      "tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Batch: 0, time: 0.28973817825317383, loss: 0.012299561873078346\n",
      "Batch: 50, time: 0.2980167865753174, loss: 0.3559548258781433\n",
      "Batch: 100, time: 0.30149245262145996, loss: 0.07394834607839584\n",
      "Batch: 150, time: 0.299511194229126, loss: 0.004851984791457653\n",
      "Batch: 200, time: 0.29958677291870117, loss: 0.05596737936139107\n",
      "Batch: 250, time: 0.30192089080810547, loss: 0.011039364151656628\n",
      "Batch: 300, time: 0.29788684844970703, loss: 0.026285648345947266\n",
      "Batch: 350, time: 0.2724928855895996, loss: 0.3243393301963806\n",
      "Batch: 400, time: 0.2723684310913086, loss: 0.024614548310637474\n",
      "Batch: 450, time: 0.2989635467529297, loss: 0.011226296424865723\n",
      "Batch: 500, time: 0.2734954357147217, loss: 0.008728718385100365\n",
      "Batch: 550, time: 0.30405616760253906, loss: 0.576679527759552\n",
      "Batch: 600, time: 0.2727475166320801, loss: 0.02402878925204277\n",
      "Batch: 650, time: 0.2936592102050781, loss: 0.02131507359445095\n",
      "Batch: 700, time: 0.29796838760375977, loss: 0.06855636835098267\n",
      "Batch: 750, time: 0.30023741722106934, loss: 0.06158344820141792\n",
      "Batch: 800, time: 0.3018794059753418, loss: 0.14810499548912048\n",
      "Batch: 850, time: 0.29886364936828613, loss: 0.018144190311431885\n",
      "Batch: 900, time: 0.3003804683685303, loss: 0.27839958667755127\n",
      "Batch: 950, time: 0.2907125949859619, loss: 0.016165828332304955\n",
      "Average loss:  0.11713282532221637\n",
      "tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Batch: 0, time: 0.2703132629394531, loss: 0.0077651022002100945\n",
      "Batch: 50, time: 0.27150750160217285, loss: 0.6618156433105469\n",
      "Batch: 100, time: 0.285888671875, loss: 0.032261498272418976\n",
      "Batch: 150, time: 0.27197813987731934, loss: 0.004567170049995184\n",
      "Batch: 200, time: 0.29809069633483887, loss: 0.011652898974716663\n",
      "Batch: 250, time: 0.2853379249572754, loss: 0.016444146633148193\n",
      "Batch: 300, time: 0.30339789390563965, loss: 0.05146782845258713\n",
      "Batch: 350, time: 0.30141448974609375, loss: 0.2171163260936737\n",
      "Batch: 400, time: 0.3002748489379883, loss: 0.1430990993976593\n",
      "Batch: 450, time: 0.27468252182006836, loss: 0.0076729776337742805\n",
      "Batch: 500, time: 0.2996857166290283, loss: 0.04929644614458084\n",
      "Batch: 550, time: 0.29840517044067383, loss: 0.6934437155723572\n",
      "Batch: 600, time: 0.2986593246459961, loss: 0.015278386883437634\n",
      "Batch: 650, time: 0.30063414573669434, loss: 0.008426571264863014\n",
      "Batch: 700, time: 0.27549123764038086, loss: 0.017065536230802536\n",
      "Batch: 750, time: 0.2832036018371582, loss: 0.03840119391679764\n",
      "Batch: 800, time: 0.300337553024292, loss: 0.03885287046432495\n",
      "Batch: 850, time: 0.30042004585266113, loss: 0.045432232320308685\n",
      "Batch: 900, time: 0.2718534469604492, loss: 0.15096722543239594\n",
      "Batch: 950, time: 0.3006441593170166, loss: 0.017150789499282837\n",
      "Average loss:  0.105335934860399\n",
      "tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(48)\n",
    "labels = torch.tensor((selected_text['label']).tolist())\n",
    "idx = np.random.permutation(selected_text.shape[0])\n",
    "\n",
    "batch_size = 10\n",
    "total_num_batch = selected_text.shape[0] // batch_size\n",
    "print(total_num_batch)\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-5)\n",
    "for i in range(10):\n",
    "    all_loss = 0\n",
    "    for batch in range(total_num_batch):\n",
    "        start = time.time() \n",
    "        optimizer.zero_grad()\n",
    "        input_ids = torch.tensor(encoded_text['input_ids'][idx[batch_size * batch:batch_size * (batch+1)], :]).to(device)\n",
    "        attention_mask = torch.tensor(encoded_text['attention_mask'][idx[batch_size * batch:batch_size * (batch+1)], :]).to(device)\n",
    "        label = torch.tensor(labels[idx[batch_size * batch:batch_size * (batch+1)]]).to(device)\n",
    "        enc = encoder(input_ids, attention_mask, labels = label)\n",
    "        loss = enc.loss\n",
    "        all_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end = time.time()\n",
    "        if batch % 50 == 0:\n",
    "            print('Batch: {}, time: {}, loss: {}'.format(batch, end-start, loss))\n",
    "    print(\"Average loss: \", all_loss / total_num_batch)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(encoder.state_dict(), \"temp/encoder.pt\")\n",
    "# encoder.config.to_json_file(\"temp/encoder_config.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DistilBertConfig.from_json_file(\"bert/encoder_config.pt\")\n",
    "encoder = DistilBertForSequenceClassification(config)\n",
    "state_dict = torch.load(\"bert/encoder.pt\")\n",
    "encoder.load_state_dict(state_dict)\n",
    "encoder = encoder.to(device)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batch:   6671\n",
      "0  Time elapsed:  2.6457459926605225\n",
      "1000  Time elapsed:  2692.9258897304535\n",
      "2000  Time elapsed:  2696.823832988739\n",
      "3000  Time elapsed:  2698.4334688186646\n",
      "4000  Time elapsed:  2693.5135509967804\n",
      "5000  Time elapsed:  2695.0844798088074\n",
      "6000  Time elapsed:  2700.5155498981476\n",
      "Finished. Time elapsed:  1810.7902421951294\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "num_batch = reviews.shape[0] // batch_size\n",
    "print(\"Total number of batch:  \", num_batch)\n",
    "all_pred = np.array([]).reshape(0, 2)\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for batch in range(num_batch):\n",
    "        encoded_reviews_batch = tokenizer(reviews.iloc[batch*batch_size:(batch+1)*batch_size]['text'].tolist(), padding='max_length', truncation=True, return_tensors='pt')\n",
    "        results = encoder(encoded_reviews_batch['input_ids'].to(device), \n",
    "                          encoded_reviews_batch['attention_mask'].to(device), \n",
    "                          labels = torch.tensor(reviews.iloc[batch*batch_size:(batch+1)*batch_size]['label'].to_list()).to(device))\n",
    "        pred = torch.nn.Softmax(dim=1)(results.logits).cpu().data.numpy()\n",
    "        all_pred = np.concatenate((all_pred, pred))\n",
    "        end = time.time()\n",
    "        if batch % 1000 == 0:\n",
    "            print(\"{}  Time elapsed:  {}\".format(batch, end - start) )\n",
    "            start = time.time()\n",
    "    encoded_reviews_batch = tokenizer(reviews.iloc[(batch+1)*batch_size:]['text'].tolist(), padding='max_length', truncation=True, return_tensors='pt')\n",
    "    results = encoder(encoded_reviews_batch['input_ids'].to(device), \n",
    "                      encoded_reviews_batch['attention_mask'].to(device), \n",
    "                      labels = torch.tensor(reviews.iloc[(batch+1)*batch_size:]['label'].to_list()).to(device))\n",
    "    pred = torch.nn.Softmax(dim=1)(results.logits).cpu().data.numpy()\n",
    "    all_pred = np.concatenate((all_pred, pred))\n",
    "    end = time.time()\n",
    "    print(\"Finished. Time elapsed:  {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[['negative', 'positive']] = all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv('reviews_with_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8499286534442791"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(reviews['label'] == all_pred.argmax(1)).sum() / reviews.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self fine tuning\n",
    "- Fix pre-training weights\n",
    "- Add dense layers on top\n",
    "- Only tune dense layers weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain embedding from pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n",
      "0 0.42522501945495605\n",
      "50 0.27116918563842773\n",
      "100 0.28605151176452637\n",
      "150 0.28875064849853516\n",
      "200 0.2722632884979248\n",
      "250 0.28982996940612793\n",
      "300 0.29233789443969727\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(48)\n",
    "idx = np.random.permutation(selected_text.shape[0])\n",
    "\n",
    "batch_size = 32\n",
    "total_num_batch = selected_text.shape[0] // batch_size\n",
    "print(total_num_batch)\n",
    "all_embedding = np.zeros(shape=(selected_text.shape[0], 768))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in range(total_num_batch):\n",
    "        start = time.time()\n",
    "        input_ids = torch.tensor(encoded_text['input_ids'][idx[batch_size * batch:batch_size * (batch+1)], :]).to(device)\n",
    "        attention_mask = torch.tensor(encoded_text['attention_mask'][idx[batch_size * batch:batch_size * (batch+1)], :]).to(device)\n",
    "        enc = encoder(input_ids, attention_mask)\n",
    "        all_embedding[idx[batch_size * batch:batch_size * (batch+1)], :] = enc[0][:,0,:].cpu().data.numpy()\n",
    "        end = time.time()\n",
    "        if batch % 50 == 0:\n",
    "            print(batch, end-start)\n",
    "    input_ids = torch.tensor(encoded_text['input_ids'][idx[batch_size * (batch+1):], :]).to(device)\n",
    "    attention_mask = torch.tensor(encoded_text['attention_mask'][idx[batch_size * (batch+1):], :]).to(device)\n",
    "    enc = encoder(input_ids, attention_mask)\n",
    "    all_embedding[idx[batch_size * (batch+1):], :] = enc[0][:,0,:].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(768, H)\n",
    "        self.linear2 = torch.nn.Linear(H, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "labels = torch.tensor((selected_text['label']).tolist())\n",
    "\n",
    "model = Model(128)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Average loss: 0.767658793200285\n",
      "Epoch: 1  Average loss: 0.6684566416228429\n",
      "Epoch: 2  Average loss: 0.62086895843729\n",
      "Epoch: 3  Average loss: 0.5883290941516558\n",
      "Epoch: 4  Average loss: 0.56410362251485\n",
      "Epoch: 5  Average loss: 0.5470325753856928\n",
      "Epoch: 6  Average loss: 0.5347868076597269\n",
      "Epoch: 7  Average loss: 0.525824815273667\n",
      "Epoch: 8  Average loss: 0.5190742890804242\n",
      "Epoch: 9  Average loss: 0.5141656713512464\n",
      "Epoch: 10  Average loss: 0.5105705606058623\n",
      "Epoch: 11  Average loss: 0.5078865797378314\n",
      "Epoch: 12  Average loss: 0.5058081769981445\n",
      "Epoch: 13  Average loss: 0.5040901992470026\n",
      "Epoch: 14  Average loss: 0.502592199553664\n",
      "Epoch: 15  Average loss: 0.5012805259858186\n",
      "Epoch: 16  Average loss: 0.500253104724181\n",
      "Epoch: 17  Average loss: 0.4992480758482065\n",
      "Epoch: 18  Average loss: 0.4984326846897602\n",
      "Epoch: 19  Average loss: 0.4976827699977618\n",
      "Epoch: 20  Average loss: 0.4969380006003074\n",
      "Epoch: 21  Average loss: 0.49625840472678345\n",
      "Epoch: 22  Average loss: 0.49557243258907246\n",
      "Epoch: 23  Average loss: 0.49499716370915753\n",
      "Epoch: 24  Average loss: 0.49436738905616295\n",
      "Epoch: 25  Average loss: 0.49380199405818415\n",
      "Epoch: 26  Average loss: 0.4932396794932011\n",
      "Epoch: 27  Average loss: 0.4926580825868325\n",
      "Epoch: 28  Average loss: 0.4920887841532628\n",
      "Epoch: 29  Average loss: 0.4915947358195598\n",
      "Epoch: 30  Average loss: 0.4910070229417238\n",
      "Epoch: 31  Average loss: 0.4905059695816957\n",
      "Epoch: 32  Average loss: 0.49003873478907806\n",
      "Epoch: 33  Average loss: 0.48959090928427684\n",
      "Epoch: 34  Average loss: 0.48912267587505853\n",
      "Epoch: 35  Average loss: 0.48866035588658774\n",
      "Epoch: 36  Average loss: 0.48822310767494714\n",
      "Epoch: 37  Average loss: 0.48779342194589287\n",
      "Epoch: 38  Average loss: 0.48733199908374214\n",
      "Epoch: 39  Average loss: 0.4869173053556528\n",
      "Epoch: 40  Average loss: 0.4864855216195186\n",
      "Epoch: 41  Average loss: 0.4860520089666049\n",
      "Epoch: 42  Average loss: 0.485671064697015\n",
      "Epoch: 43  Average loss: 0.485242550380719\n",
      "Epoch: 44  Average loss: 0.4848373010276984\n",
      "Epoch: 45  Average loss: 0.4844548274309207\n",
      "Epoch: 46  Average loss: 0.48409131575280273\n",
      "Epoch: 47  Average loss: 0.48366531930290735\n",
      "Epoch: 48  Average loss: 0.48335057458816433\n",
      "Epoch: 49  Average loss: 0.48299272539906013\n",
      "Epoch: 50  Average loss: 0.48263480733984554\n",
      "Epoch: 51  Average loss: 0.4822237887061559\n",
      "Epoch: 52  Average loss: 0.4818765139923646\n",
      "Epoch: 53  Average loss: 0.4815179018829113\n",
      "Epoch: 54  Average loss: 0.48116864163715106\n",
      "Epoch: 55  Average loss: 0.48084069788455963\n",
      "Epoch: 56  Average loss: 0.4804768200772695\n",
      "Epoch: 57  Average loss: 0.48014631752784437\n",
      "Epoch: 58  Average loss: 0.47978227781370664\n",
      "Epoch: 59  Average loss: 0.4794919790747838\n",
      "Epoch: 60  Average loss: 0.47914290967851114\n",
      "Epoch: 61  Average loss: 0.4788002324027893\n",
      "Epoch: 62  Average loss: 0.4784877263964751\n",
      "Epoch: 63  Average loss: 0.4781777266030892\n",
      "Epoch: 64  Average loss: 0.47783286344164455\n",
      "Epoch: 65  Average loss: 0.4775156678679662\n",
      "Epoch: 66  Average loss: 0.47724594428944284\n",
      "Epoch: 67  Average loss: 0.4769418666091485\n",
      "Epoch: 68  Average loss: 0.4766314729857139\n",
      "Epoch: 69  Average loss: 0.4763143304257821\n",
      "Epoch: 70  Average loss: 0.4760189608981212\n",
      "Epoch: 71  Average loss: 0.47574877600448257\n",
      "Epoch: 72  Average loss: 0.4754445353188576\n",
      "Epoch: 73  Average loss: 0.4751763818069146\n",
      "Epoch: 74  Average loss: 0.4748853369114491\n",
      "Epoch: 75  Average loss: 0.4745830612686964\n",
      "Epoch: 76  Average loss: 0.4743064603744409\n",
      "Epoch: 77  Average loss: 0.4739937978104139\n",
      "Epoch: 78  Average loss: 0.4736917095306592\n",
      "Epoch: 79  Average loss: 0.4734573472195711\n",
      "Epoch: 80  Average loss: 0.4731609715292087\n",
      "Epoch: 81  Average loss: 0.47290777338620943\n",
      "Epoch: 82  Average loss: 0.47271454248290795\n",
      "Epoch: 83  Average loss: 0.47244884522679526\n",
      "Epoch: 84  Average loss: 0.4721624734214483\n",
      "Epoch: 85  Average loss: 0.4719518717282858\n",
      "Epoch: 86  Average loss: 0.4716510201493899\n",
      "Epoch: 87  Average loss: 0.47141832686387575\n",
      "Epoch: 88  Average loss: 0.4711311783355016\n",
      "Epoch: 89  Average loss: 0.47089330235926\n",
      "Epoch: 90  Average loss: 0.4706759193482307\n",
      "Epoch: 91  Average loss: 0.47037258066045934\n",
      "Epoch: 92  Average loss: 0.47012473652378106\n",
      "Epoch: 93  Average loss: 0.4698954622905988\n",
      "Epoch: 94  Average loss: 0.4696494726798473\n",
      "Epoch: 95  Average loss: 0.46937744768384176\n",
      "Epoch: 96  Average loss: 0.46916768413323623\n",
      "Epoch: 97  Average loss: 0.46892439564451194\n",
      "Epoch: 98  Average loss: 0.46865315343707037\n",
      "Epoch: 99  Average loss: 0.4684047297311899\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ad7ef27ecb82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mall_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "for i in range(1000):\n",
    "    all_loss = 0\n",
    "    for batch in range(total_num_batch):\n",
    "        start = time.time() \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(torch.tensor(all_embedding[batch*batch_size:(batch+1)*batch_size, :]).float().to(device))\n",
    "        label = torch.tensor(labels[idx[batch_size * batch:batch_size * (batch+1)]])\n",
    "        loss = criterion(pred.cpu(), label.float().unsqueeze(1))\n",
    "        all_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end = time.time()\n",
    "    print(\"Epoch: {}  Average loss: {}\".format(i, float(all_loss / total_num_batch)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
